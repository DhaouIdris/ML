{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3fiaKbtLmoY"
      },
      "source": [
        "To download the images of the dataset, you need to download them from our google drive because stanford stopped hosting them (we used to do `! wget http://ai.stanford.edu/~jkrause/car196/car_ims.tgz`).\n",
        "\n",
        "[the link](https://drive.google.com/file/d/1NuxNKiw7MXEdXqVVBtBsDv38QNAh2QHT/view?usp=share_link)\n",
        "\n",
        "Then, put it in the same spot as the notebook. To extract it and download the csv's, you can run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRGXEBc9iM5f"
      },
      "outputs": [],
      "source": [
        "! tar xzf car_ims.tgz\n",
        "\n",
        "! wget \"https://storage.googleapis.com/monk-public/exercice/car_classification/03_05_2022_update/test_dataset.csv\" -O test_dataset.csv -nv\n",
        "! wget \"https://storage.googleapis.com/monk-public/exercice/car_classification/03_05_2022_update/train_dataset.csv\" -O train_dataset.csv -nv\n",
        "! wget \"https://storage.googleapis.com/monk-public/exercice/car_classification/03_05_2022_update/validation_dataset.csv\" -O validation_dataset.csv -nv\n",
        "! wget \"https://storage.googleapis.com/monk-public/exercice/car_classification/03_05_2022_update/class_names_dict.pkl\" -O class_names_dict.pkl -nv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7cwSE2TKUfr"
      },
      "source": [
        "The annotations are split into train/validation/test sets and can be found in the exercise ressources under:\n",
        "- train_dataset.csv\n",
        "- validation_dataset.csv\n",
        "- test_dataset.csv\n",
        "\n",
        "Annotations are saved as csv files. You can load them using the 'read_csv' method of the pandas library.\n",
        "\n",
        "Class names are stored under the file : 'class_names_dict.pkl' you can load it using the standard pickle library.\n",
        "\n",
        "There are 196 car models in the dataset such as:\n",
        "\n",
        "```\n",
        "[\n",
        "  'Rolls-Royce Ghost Sedan 2012',\n",
        "  'BMW X6 SUV 2012',\n",
        "  'Jeep Liberty SUV 2012',\n",
        "  ...,\n",
        "]\n",
        "```\n",
        "\n",
        "Each image is associated with a class id and a bounding box (x1,x2,y1,y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAMJoIZSAvWg"
      },
      "source": [
        "We will then proceed in three parts:\n",
        "\n",
        "*   Classification using Transfer learning: load a pretrained model and finetune on the training data. We do so because our time is limited and we want to leverage the power of some very accurate models already existing and open source.\n",
        "\n",
        "*  Object detection: use the YOLO model for the car detection and the bounding box (as for the previous point, this is a well known and very performant model)\n",
        "\n",
        "*   Ensemble Modelling: Merge both to one algorithm that, given two images will draw both bouding box and tells if it is the same car or not\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yON-M_K9aaHt"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2vrmI3zacc3",
        "outputId": "1bbc4f99-ae85-4f87-fad3-aa494d3e931c"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install torchvision\n",
        "!{sys.executable} -m pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp-0jWOkajga"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import cv2\n",
        "import pickle\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from tensorflow.keras.models import load_model, Sequential, Model\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input, VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojXQDHqzavNG"
      },
      "source": [
        "I uploaded car_ims on drive because the downloading on colab was not working well (ignore the two following code boxes if download works)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zmf6KpmlW4h_",
        "outputId": "0e32297c-7276-4af9-ff77-c52204a935bb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_zhIU8fiFGX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3EIXwO1apiK"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0yIxevOaxXr"
      },
      "source": [
        "Creating two dataframes with adapted values for class columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHCtuE32atyJ"
      },
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "train_df = pd.read_csv('train_dataset.csv')\n",
        "test_df = pd.read_csv('test_dataset.csv')\n",
        "\n",
        "#The class in training and test were not really consistent, with negative values. changing them ranging from 0 to 196\n",
        "train_df['class'] = train_df['class'].apply(lambda x: x - 1 if x > 0 else x + 255)\n",
        "train_df['class'] = train_df['class'].astype(str)\n",
        "test_df['class'] = test_df['class'].apply(lambda x: x - 1 if x > 0 else x + 255)\n",
        "test_df['class'] = test_df['class'].astype(str)\n",
        "\n",
        "# Save updated datasets\n",
        "train_df.to_csv('updated_train.csv', index=False)\n",
        "test_df.to_csv('updated_test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbX2lBU7bM2b"
      },
      "source": [
        "In order to improve the robustness and generalization ability of the model, we employ data augmentation techniques on the training dataset. This helps the model learn to better handle variations and increases its ability to accurately classify unseen examples.\n",
        "\n",
        "We apply preprocessing to both the training and test datasets for consistency and optimal model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZkY4IUVbOOc"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W6xW_bqbdR8"
      },
      "source": [
        "#### Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4kSxZjXbgpb",
        "outputId": "2a018550-3407-4c36-a533-fdf0d5678427"
      },
      "outputs": [],
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    directory=None,\n",
        "    x_col='relative_im_path',\n",
        "    y_col='class',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pj0bR9kbj1_",
        "outputId": "533b3334-a89d-4a30-86f4-1a93594fcec0"
      },
      "outputs": [],
      "source": [
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    directory=None,\n",
        "    x_col='relative_im_path',\n",
        "    y_col='class',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arDZ73UjbufW"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28jVARo7bwfd"
      },
      "source": [
        "In this section, we leverage the power of a pretrained VGG16 model for our task. VGG16 is a convolutional neural network architecture that has been pre-trained on the ImageNet dataset, making it capable of recognizing a wide range of visual concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzGNlE5agMc0"
      },
      "source": [
        "#### Loading the Pretrained Model\n",
        "\n",
        "We first import VGG16 model with pre-trained weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxZOY0nYXE2x",
        "outputId": "8e0c16a8-6f5d-40b7-cb21-675f6427c317"
      },
      "outputs": [],
      "source": [
        "#Model config\n",
        "IMAGE_SIZE = [224, 224]\n",
        "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQqaoWu5b-vj"
      },
      "source": [
        "#### Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIux5V8HXE97"
      },
      "outputs": [],
      "source": [
        "#Transfer Learning\n",
        "\n",
        "#freeze the base layers:\n",
        "for layer in vgg.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add new layers on top of the pretrained base\n",
        "x = Flatten()(vgg.output)\n",
        "\n",
        "prediction = Dense(196, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=vgg.input, outputs=prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5EvzT-4cDGw"
      },
      "source": [
        "VGG16 consists of a series of convolutional and max-pooling layers followed by fully connected layers. We added a dense layer at the end in order to adapt classification to our problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTsYaa7xiTB_"
      },
      "source": [
        "Summary of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPWLPzmUXamH",
        "outputId": "f69f620e-f355-4a71-eb64-dce1c7a4d550"
      },
      "outputs": [],
      "source": [
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5c0ryrSbOFu"
      },
      "source": [
        "For the training, we use CrossEntropy loss that allows to handle multi-class classification tasks and the Adam optimizer ensuring efficient convergence during training by adatpting the learning rate.\n",
        "\n",
        "Accuracy is the preferred metric for VGG16 classification models because it gives a quick and clear measure of how well the model correctly identifies images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7879WXU2Xar-"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua_zzI5KdOQ4"
      },
      "source": [
        "Training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePt_am6yYGHV",
        "outputId": "d3a820ba-d1bc-49fd-8ef3-c1a2376cfc25"
      },
      "outputs": [],
      "source": [
        "r = model.fit_generator(\n",
        "  train_generator,\n",
        "  validation_data=test_generator,\n",
        "  epochs=30,\n",
        "  steps_per_epoch=len(train_generator),\n",
        "  validation_steps=len(test_generator)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLd6_6HSdV4w"
      },
      "source": [
        "#### Results Analysis\n",
        "\n",
        "Visualizing essential metrics, like accuracy and loss, to gain insights into the model's performance and identify areas for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "oKDfoYVMYGNA",
        "outputId": "55b32c36-bf06-4e72-c24c-82f068d12226"
      },
      "outputs": [],
      "source": [
        "# plot the loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "\n",
        "# plot the accuracy\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjklx9HVdtKi"
      },
      "source": [
        "Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsTxpun3oTri"
      },
      "outputs": [],
      "source": [
        "model.save('model_vgg.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uQSTKADcU40"
      },
      "source": [
        "### Object Detection\n",
        "\n",
        "The YOLO package is a state-of-the-art model for object detection in images, providing localization and classification of objects with high efficiency and accuracy. We use this model for box identification as for this part, we do not look for specific models but we rather want to identify a car.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRhtTzMudF1o"
      },
      "source": [
        "#### Loading the Pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEX5GIKndpWV",
        "outputId": "cdec02cc-a522-416b-eb3f-96d0f9cc3bd1"
      },
      "outputs": [],
      "source": [
        "#Model Loading\n",
        "model1 = YOLO('yolov8n.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTh8bANZdWHB"
      },
      "source": [
        "#### Box Selection\n",
        "\n",
        "This model creates a box for every object it detects so I will filter by selecting the biggest box by size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXlq7Zptcpid"
      },
      "outputs": [],
      "source": [
        "#calculate the area for a box\n",
        "def calculate_box_area(box):\n",
        "    return (box[2] - box[0]) * (box[3] - box[1])\n",
        "\n",
        "#choosing only the biggest box given a list of boxes by size\n",
        "def filter_largest_box(boxes):\n",
        "    if not boxes:\n",
        "        return None\n",
        "\n",
        "    biggest_box = boxes[0]\n",
        "    max_area = calculate_box_area(biggest_box)\n",
        "\n",
        "    for box in boxes[1:]:\n",
        "        area = calculate_box_area(box)\n",
        "        if area > max_area:\n",
        "            max_area = area\n",
        "            biggest_box = box\n",
        "\n",
        "    return biggest_box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BxXCScqeAN0"
      },
      "outputs": [],
      "source": [
        "#drawing the box on the jpg\n",
        "def draw_bounding_box(image_path, box, title):\n",
        "    image = plt.imread(image_path)\n",
        "\n",
        "    fig, ax = plt.subplots(1)\n",
        "    ax.imshow(image)\n",
        "\n",
        "    rect = patches.Rectangle(\n",
        "        (box[0], box[1]),\n",
        "        box[2] - box[0],\n",
        "        box[3] - box[1],\n",
        "        linewidth=2,\n",
        "        edgecolor='r',\n",
        "        facecolor='none'\n",
        "    )\n",
        "\n",
        "\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bJMJsJWhOBZ"
      },
      "source": [
        "Loading the dictionnary for the cars models name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vw0OEXXghS3c",
        "outputId": "9efb9d0f-6846-4c31-af6c-f6aab0509ac4"
      },
      "outputs": [],
      "source": [
        "with open('class_names_dict.pkl', 'rb') as file:\n",
        "    models = pickle.load(file)\n",
        "\n",
        "print(list(models.keys())[list(models.values()).index(58)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_XwFRawdzkq"
      },
      "source": [
        "#### Detection example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne5C79VLeFrJ"
      },
      "source": [
        "Exemple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "xNGJQF8MeC5Q",
        "outputId": "dcad299b-e528-4e66-c64e-eeb72dcfca5c"
      },
      "outputs": [],
      "source": [
        "img = train_df['relative_im_path'][100]\n",
        "results = model1(img, show = False)\n",
        "boxes = results[0].boxes.xyxy.tolist()\n",
        "largest_box = filter_largest_box(boxes)\n",
        "print(boxes)\n",
        "print(largest_box)\n",
        "\n",
        "draw_bounding_box(img, largest_box,f\"{list(models.keys())[list(models.values()).index(train_df['class'][100])]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiBrKssBd5t7"
      },
      "source": [
        "The bounding box is correctly delimiting the car given a jpg file, we will merge both parts so that it detects the car model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wIGBcx9i1Js"
      },
      "outputs": [],
      "source": [
        "def main(img1, img2):\n",
        "\n",
        "  img1=image.load_img('img1',target_size=(224,224))\n",
        "  img2=image.load_img('img2',target_size=(224,224))\n",
        "  img1=image.img_to_array(img1)/255\n",
        "  img2=image.img_to_array(img2)/255\n",
        "\n",
        "  img1=np.expand_dims(img1,axis=0)\n",
        "  img1_data=preprocess_input(img1)\n",
        "  img1_data.shape\n",
        "  img2=np.expand_dims(img2,axis=0)\n",
        "  img2_data=preprocess_input(img2)\n",
        "  img2_data.shape\n",
        "\n",
        "\n",
        "  if np.argmax(model.predict(img1_data), axis=1)==np.argmax(model.predict(img2_data), axis=1):\n",
        "    print(f'The cars are the same model: {list(models.keys())[list(models.values()).index(np.argmax(model.predict(img1_data), axis=1)+1)]}') #We ranges the labels from 0 to 195 but it is from 1 to 196 in the dictionnary\n",
        "  else:\n",
        "    print('The cars are different models')\n",
        "\n",
        "  results1 = model1(img1, show = False)\n",
        "  boxes = results[0].boxes.xyxy.tolist()\n",
        "  largest_box = filter_largest_box(boxes)\n",
        "  draw_bounding_box(img, largest_box, f'First car model :{list(models.keys())[list(models.values()).index(np.argmax(model.predict(img1_data), axis=1)+1)]}')\n",
        "  print(\"Coordinates of the box of the first car:\")\n",
        "  print(f\"x1: {largest_box[0]}, y1: {largest_box[1]}\")\n",
        "  print(f\"x2: {largest_box[2]}, y2: {largest_box[3]}\")\n",
        "\n",
        "  results = model1(img1, show = False)\n",
        "  boxes = results[0].boxes.xyxy.tolist()\n",
        "  largest_box = filter_largest_box(boxes)\n",
        "  draw_bounding_box(img, largest_box, f'Second car model :{list(models.keys())[list(models.values()).index(np.argmax(model.predict(img2_data), axis=1)+1)]}')\n",
        "  print(\"Coordinates of the box of the second car:\")\n",
        "  print(f\"x1: {largest_box[0]}, y1: {largest_box[1]}\")\n",
        "  print(f\"x2: {largest_box[2]}, y2: {largest_box[3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsX5mtW_rdPK"
      },
      "source": [
        "# **Conclusion**\n",
        "\n",
        "---\n",
        "\n",
        "Please provide a critical analysis of your approach and potential next steps you can think of.\n",
        "\n",
        "\n",
        "I did not train the model properly due to GPU limitation on colab, I wish I could train it on more epochs.\n",
        "Moreover, the datasets provided do not have as much data as I would have wanted, at first I wanted to use Visual transformers but it is really data hungry so it did not give results good enough.\n",
        "\n",
        "For the bounding box solution I provided, I think that just choosing the biggest box is not 100% fool proof. I did not have a lot of time left, it was the only idea I had at the moment but I am sure there should be a better solution. Same thing for the part B in exerice 1, I did it last and did not have enough time for the last question."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0DiXlGnnEvPi",
        "fO9HwIfwDr5E"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
